{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiRajesh228/DA6401_Assignment3/blob/main/DA6401A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmLdw9IANSdu",
        "outputId": "cdcb179e-8be5-4867-d80f-758daf43bd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transliteration_project\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Setup complete! Files created:\n",
            "total 64\n",
            "drwxr-xr-x 3 root root  4096 May 20 05:09 .\n",
            "drwxr-xr-x 1 root root  4096 May 20 05:09 ..\n",
            "-rw-r--r-- 1 root root  5793 May 20 05:09 Core_Utils.py\n",
            "drwxr-xr-x 3 root root  4096 May 20 05:09 data_folder\n",
            "-rw-r--r-- 1 root root  9177 May 20 05:09 Encoder_Decoder_Architecture.py\n",
            "-rw-r--r-- 1 root root 11761 May 20 05:09 Machine_Translator.py\n",
            "-rw-r--r-- 1 root root  8843 May 20 05:09 trainer.py\n",
            "-rw-r--r-- 1 root root  4216 May 20 05:09 visualize_attention.py\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transliteration_project/trainer.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 216, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Single-cell version of Sequence-to-Sequence Transliteration Project\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Create project directory\n",
        "!mkdir -p /content/transliteration_project\n",
        "%cd /content/transliteration_project\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q pandas numpy torch matplotlib wandb tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize wandb (optional)\n",
        "import wandb\n",
        "wandb.login()  # Uncomment if you want to use wandb\n",
        "\n",
        "# Write Core_Utils.py\n",
        "with open('Core_Utils.py', 'w') as f:\n",
        "    f.write('''\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import gc\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "seed = 23\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class LanguageProcessor:\n",
        "    def __init__(self,language_directory,target_lang_name,mode=\"train\",meta_tokens=True):\n",
        "        \"\"\"\n",
        "        Default Constructor for this class.\n",
        "        Params:\n",
        "            language_directory : ex : \"aksharantar_sampled/tel/\"\n",
        "            mode : \"train\" or \"test\" or \"valid\", accordingly the appropriate dataset is read.\n",
        "            meta_tokens : If true creates the first three tokens of the dictionary as <start>,<end>,<pad>.\n",
        "        \"\"\"\n",
        "        self.meta_tokens = meta_tokens\n",
        "        self.language_directory = language_directory\n",
        "        self.target_lang_name = target_lang_name\n",
        "        self.mode = mode\n",
        "        self.source_lang = 0\n",
        "        self.target_lang = 1\n",
        "        self.source_char2id,self.source_id2char = self.build_char_vocab(self.source_lang)\n",
        "        self.target_char2id,self.target_id2char = self.build_char_vocab(self.target_lang)\n",
        "\n",
        "    def build_char_vocab(self,lang_id,max_len=None):\n",
        "        \"\"\"\n",
        "        Method to create a vocabulary of characters in language corresponding to lang_id.\n",
        "        \"\"\"\n",
        "        # Modified to ensure all data is read as strings and handle tab separators\n",
        "        df = pd.read_csv(self.language_directory+self.mode+\".txt\",\n",
        "                         header=None,\n",
        "                         sep='\\\\t',\n",
        "                         dtype=str,  # Enforce string type for all columns\n",
        "                         na_filter=False)  # Don't interpret empty fields as NaN\n",
        "\n",
        "        # Only keep the first two columns for source and target languages\n",
        "        if df.shape[1] > 2:\n",
        "            df = df.iloc[:, 0:2]\n",
        "\n",
        "        self.data = df.to_numpy()\n",
        "\n",
        "        lang_chars = []\n",
        "        lang_words = df[lang_id].to_numpy()\n",
        "\n",
        "        for word in lang_words:\n",
        "            # Make sure word is treated as a string\n",
        "            if word is not None and word != '':\n",
        "                lang_chars += list(str(word))\n",
        "\n",
        "        unique_lang_chars = sorted(list(set(lang_chars)))\n",
        "\n",
        "        if self.meta_tokens:\n",
        "            char2id_dict = {'<start>':0,'<end>':1,'<pad>': 2}\n",
        "            id2char_dict = {0:'<start>',1:'<end>',2:'<pad>'}\n",
        "            self.start_token_id = char2id_dict['<start>']\n",
        "            self.end_token_id = char2id_dict['<end>']\n",
        "            self.pad_token_id = char2id_dict['<pad>']\n",
        "        else:\n",
        "            char2id_dict = {}\n",
        "            id2char_dict = {}\n",
        "\n",
        "        start = len(char2id_dict)\n",
        "\n",
        "        for i in range(len(unique_lang_chars)):\n",
        "            char2id_dict[unique_lang_chars[i]] = i+start\n",
        "            id2char_dict[i+start] = unique_lang_chars[i]\n",
        "\n",
        "        del df\n",
        "        del lang_chars\n",
        "        del unique_lang_chars\n",
        "        gc.collect()\n",
        "\n",
        "        return char2id_dict,id2char_dict\n",
        "\n",
        "    def encode_word(self,word,lang_id,padding=False,append_eos = False):\n",
        "        \"\"\"\n",
        "        Method to encode characters of a given word.\n",
        "        \"\"\"\n",
        "        if lang_id == self.source_lang:\n",
        "            char2id_dict = self.source_char2id\n",
        "        else:\n",
        "            char2id_dict = self.target_char2id\n",
        "\n",
        "        word_encoding = []\n",
        "        # Ensure we're working with a string\n",
        "        word = str(word).lower()\n",
        "\n",
        "        for i in word:\n",
        "            word_encoding.append(char2id_dict[i])\n",
        "\n",
        "        if append_eos:\n",
        "            word_encoding.append(char2id_dict['<end>'])\n",
        "\n",
        "        return np.array(word_encoding)\n",
        "\n",
        "    def decode_word(self,code_word,lang_id):\n",
        "        \"\"\"\n",
        "        Method to decode an encoded word.\n",
        "        \"\"\"\n",
        "        word = []\n",
        "\n",
        "        if lang_id == self.source_lang:\n",
        "            id2char_dict = self.source_id2char\n",
        "            char2id_dict = self.source_char2id\n",
        "        else:\n",
        "            id2char_dict = self.target_id2char\n",
        "            char2id_dict = self.target_char2id\n",
        "\n",
        "        start_idx = 0\n",
        "\n",
        "        for i in code_word[start_idx:]:\n",
        "            ## if we reached <end>, then stop decoding\n",
        "            if self.meta_tokens and i == char2id_dict['<end>'] or i == char2id_dict['<pad>']:\n",
        "                break\n",
        "            word.append(id2char_dict[i])\n",
        "\n",
        "        return np.array(word)\n",
        "\n",
        "class WordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that inherits and overrides the methods of Dataset class.\n",
        "    \"\"\"\n",
        "    def __init__(self, language_processor,append_eos=True,device='cpu'):\n",
        "        self.lp = language_processor\n",
        "        self.data = self.lp.data\n",
        "        self.device = device\n",
        "        self.append_eos = append_eos\n",
        "        self.start_token_id = self.lp.start_token_id\n",
        "        self.end_token_id = self.lp.end_token_id\n",
        "        self.pad_token_id = self.lp.pad_token_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_word, output_word = self.data[idx]\n",
        "        input_sequence = self.lp.encode_word(input_word,self.lp.source_lang,padding=False,append_eos=self.append_eos)\n",
        "        output_sequence = self.lp.encode_word(output_word,self.lp.target_lang,padding=False,append_eos=self.append_eos)\n",
        "        return torch.tensor(input_sequence).to(self.device), torch.tensor(output_sequence).to(self.device)\n",
        "\n",
        "def collate_fn(batch,pad_token_id,device):\n",
        "    \"\"\"\n",
        "    The method to collate on a batch of data, by adding padding based on the longest string in the batch.\n",
        "    \"\"\"\n",
        "    input_words, target_words = zip(*batch)\n",
        "\n",
        "    padded_inputs = pad_sequence(input_words, batch_first=True, padding_value=pad_token_id)\n",
        "    padded_targets = pad_sequence(target_words, batch_first=True, padding_value=pad_token_id)\n",
        "\n",
        "    input_lengths = torch.LongTensor([len(seq) for seq in input_words]).to(device)\n",
        "    target_lengths = torch.LongTensor([len(seq) for seq in target_words]).to(device)\n",
        "\n",
        "    return padded_inputs, padded_targets, input_lengths, target_lengths\n",
        "''')\n",
        "\n",
        "# Write Encoder_Decoder_Architecture.py\n",
        "with open('Encoder_Decoder_Architecture.py', 'w') as f:\n",
        "    f.write('''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seed = 23\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The class to implement Additive attention aka Bhadanau Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size,D,expected_dim,batch_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.U_att = nn.Linear(hidden_size*expected_dim, hidden_size)\n",
        "        self.W_att = nn.Linear(hidden_size*D, hidden_size)\n",
        "        self.V_att = nn.Linear(hidden_size, 1)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, decoder_prev_hidden, encoder_contexts):\n",
        "      # Get batch size from the input tensor instead of using self.batch_size\n",
        "        batch_size = encoder_contexts.size(0)\n",
        "\n",
        "        decoder_prev_hidden = decoder_prev_hidden.reshape(batch_size, 1, -1)\n",
        "        scores = self.V_att(torch.tanh(self.U_att(decoder_prev_hidden) + self.W_att(encoder_contexts))).squeeze(2).unsqueeze(1)\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, encoder_contexts)\n",
        "        return context, weights\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The class that implements the encoder using Recurrent Units RNN/LSTM/GRU.\n",
        "    \"\"\"\n",
        "    def __init__(self, source_vocab_size,hidden_size,embedding_size,rnn_type = \"GRU\",padding_idx = None ,dropout=0.1,num_layers = 1,bidirectional = False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = self.hidden_size\n",
        "        self.rnn_type = rnn_type\n",
        "        self.D = 1 ##the number of directions in which the input is viewed.\n",
        "        if bidirectional:\n",
        "            self.D = 2\n",
        "        self.rnn_dropout = 0\n",
        "        if self.num_layers>1:\n",
        "            self.rnn_dropout = dropout\n",
        "        self.embedding = nn.Embedding(source_vocab_size, self.embedding_size,padding_idx = padding_idx)\n",
        "\n",
        "        if self.rnn_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(self.embedding_size, self.hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
        "        elif self.rnn_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(self.embedding_size, self.hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
        "        elif self.rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input,hidden=None,cell=None):\n",
        "        input_embedding = self.dropout(self.embedding(input))\n",
        "        if self.rnn_type == \"LSTM\":\n",
        "            output, (hidden, cell) = self.rnn(input_embedding, (hidden, cell))\n",
        "        else:\n",
        "            output, hidden = self.rnn(input_embedding)\n",
        "            cell = None\n",
        "        return output, hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The class to implement Decoder in the encoder-decoder architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size,embedding_size,target_vocab_size,rnn_type,batch_size,use_attention = True,padding_idx = None,num_layers = 1,bidirectional = False,dropout=0,device = \"cpu\"):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_type = rnn_type\n",
        "        self.device = device\n",
        "        self.D = 1\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = self.hidden_size\n",
        "        self.use_attention = use_attention\n",
        "        if bidirectional:\n",
        "            self.D = 2\n",
        "        ## In h0 (the input to the decoder) first dimension expected is number of directions X number of layers\n",
        "        self.expected_h0_dim1 = self.D*self.num_layers\n",
        "        ##create an embedding layer, and ignore padding index\n",
        "        if self.use_attention:\n",
        "            factor = self.D\n",
        "        else:\n",
        "            factor = 1\n",
        "        self.embedding = nn.Embedding(target_vocab_size, self.embedding_size*factor,padding_idx = padding_idx)\n",
        "        if self.use_attention:\n",
        "            self.attention = BahdanauAttention(self.hidden_size,self.D,self.expected_h0_dim1,batch_size)\n",
        "            recurrent_unit_input_dim = 2*self.D*self.hidden_size\n",
        "        else:\n",
        "            recurrent_unit_input_dim = self.embedding_size\n",
        "        self.rnn_dropout = 0\n",
        "        if self.num_layers>1:\n",
        "            self.rnn_dropout = dropout\n",
        "        if self.rnn_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(recurrent_unit_input_dim, self.hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
        "        elif self.rnn_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(recurrent_unit_input_dim, self.hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
        "        elif self.rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(recurrent_unit_input_dim, self.hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
        "        ## Passing the hidden state through a fully conencted layer and then applying softmax\n",
        "        self.output_layer = nn.Linear(self.hidden_size*self.D, target_vocab_size)\n",
        "\n",
        "    def forward(self, encoder_hidden_contexts, encoder_last_hidden,encoder_cell,target_tensor,eval_mode = False,teacher_forcing_ratio=0):\n",
        "        batch_size = encoder_hidden_contexts.size(0)\n",
        "        if not eval_mode:\n",
        "            max_word_len = target_tensor.size(1)\n",
        "        ## eval mode is for looking at a specific word that is predicted to compare with the correct word.\n",
        "        if eval_mode:\n",
        "            batch_size = 1\n",
        "            max_word_len = 30 ## an arbitrary number, larger in expecected sense.\n",
        "\n",
        "        decoder_outputs = []\n",
        "        if self.use_attention:\n",
        "            attentions = []\n",
        "        else:\n",
        "            attentions = None\n",
        "\n",
        "        ## At the first time step < SOS > token (which has an id 0, is fed as an input to the decoder).\n",
        "        decoder_input = torch.zeros((batch_size, 1), dtype=torch.long, device=self.device)\n",
        "        decoder_hidden = encoder_last_hidden ## in the first time step of the decoder, the output of the encoder is the input.\n",
        "        decoder_cell = encoder_cell ## the cell state, which is initially same as that of encoder, (applies to LSTM unit only)\n",
        "\n",
        "        for step in range(max_word_len):\n",
        "            ## eval mode is for looking at a specific word that is predicted to compare with the correct word.\n",
        "            if eval_mode:\n",
        "                decoder_input = decoder_input.view(1,-1)\n",
        "\n",
        "            embedding = self.embedding(decoder_input)\n",
        "\n",
        "            if decoder_hidden.shape[0] != self.expected_h0_dim1:\n",
        "                reshaped_hidden = decoder_hidden.repeat(self.expected_h0_dim1,1,1)\n",
        "            else:\n",
        "                reshaped_hidden = decoder_hidden\n",
        "\n",
        "            if self.use_attention:\n",
        "                ## the attention part.\n",
        "                decoder_prev_hidden = reshaped_hidden.permute(1, 0, 2)\n",
        "                context_vector, attention_weights = self.attention(decoder_prev_hidden, encoder_hidden_contexts)\n",
        "                tmp_input = torch.cat((embedding, context_vector), dim=2)\n",
        "            else:\n",
        "                ## introducing non-lineartiy through ReLU activation\n",
        "                activated_embedding = F.relu(embedding)\n",
        "                tmp_input = activated_embedding\n",
        "\n",
        "            if self.rnn_type == \"LSTM\":\n",
        "                tmp_output, (decoder_hidden, decoder_cell) = self.rnn(tmp_input, (reshaped_hidden, decoder_cell))\n",
        "            else:\n",
        "                tmp_output, decoder_hidden = self.rnn(tmp_input, reshaped_hidden)\n",
        "                cell = None\n",
        "\n",
        "            decoder_output = self.output_layer(tmp_output.squeeze(0))\n",
        "\n",
        "            ## randomly sample a number in (0,1) and if the number is less than the teacher forcing ratio\n",
        "            ## apply teacher forcing at the current step\n",
        "            apply_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            if (target_tensor is not None) and (apply_teacher_forcing):\n",
        "                ## Teacher forcing: Feed the target as the next input\n",
        "                ## extract the 't'th token from th target string to feed as input at \"t\"th time step.\n",
        "                decoder_input = target_tensor[:, step].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                ##greedily pick predictions, i.e pick the character corresponding to the hightest probability\n",
        "                _,preds = torch.max(decoder_output,dim=2)\n",
        "                decoder_input = preds.detach()\n",
        "\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            if self.use_attention:\n",
        "                attentions.append(attention_weights)\n",
        "\n",
        "        ## concatenate the predictions across all the timesteps into a singel tensor\n",
        "        ## found in literature that log_softmax does better than softmax, hence going with that.\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "\n",
        "        ## the idea is to have a common API for both attention and normal decoder, achiveing ease of use.\n",
        "        return decoder_outputs, decoder_hidden,attentions\n",
        "''')\n",
        "\n",
        "# Write Machine_Translator.py - THIS IS THE MODIFIED SECTION\n",
        "with open('Machine_Translator.py', 'w') as f:\n",
        "    f.write('''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch import optim\n",
        "import wandb\n",
        "\n",
        "from Encoder_Decoder_Architecture import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "seed = 23\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class MachineTranslator:\n",
        "    \"\"\"\n",
        "    The class that instantiates the encoder-decoder architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self,source_vocab_size,target_vocab_size,hidden_size,embedding_size,rnn_type,batch_size,pad_token_id,dropout,num_layers,bidirectional,use_attention,device):\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(source_vocab_size = source_vocab_size, hidden_size = hidden_size,embedding_size=embedding_size,rnn_type = rnn_type,padding_idx=pad_token_id,num_layers=num_layers,bidirectional=bidirectional,dropout=dropout).to(self.device)\n",
        "        self.decoder = Decoder(hidden_size = hidden_size,embedding_size=embedding_size, target_vocab_size = target_vocab_size,batch_size = batch_size,rnn_type = rnn_type,use_attention = use_attention, padding_idx = pad_token_id,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout,device=self.device).to(self.device)\n",
        "\n",
        "    def train_epoch(self,train_loader, encoder, decoder, encoder_optim,decoder_optim, loss_criterion,teacher_forcing_ratio,ignore_padding=True,device='cpu'):\n",
        "        \"\"\"\n",
        "        Method to train the encoder-decoder model for 1 epoch.\n",
        "        \"\"\"\n",
        "        tot_correct_word_preds = 0\n",
        "        tot_words = 0\n",
        "        epoch_loss = 0\n",
        "\n",
        "        total_batches = len(train_loader)\n",
        "        print(f\"Training on {total_batches} batches\")\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            # Only print every 10% progress\n",
        "            if batch_idx % max(1, total_batches // 10) == 0:\n",
        "                print(f\"Batch progress: {batch_idx}/{total_batches} ({int(100 * batch_idx / total_batches)}%)\")\n",
        "\n",
        "            input_tensor, target_tensor,_,_ = data\n",
        "            encoder_optim.zero_grad()\n",
        "            decoder_optim.zero_grad()\n",
        "            batch_size = data[0].shape[0]\n",
        "\n",
        "            if encoder.rnn_type == \"LSTM\":\n",
        "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
        "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
        "            else:\n",
        "                encoder_hidden = None\n",
        "                encoder_cell = None\n",
        "\n",
        "            encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
        "            decoder_outputs, _, _ = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor=target_tensor,teacher_forcing_ratio = teacher_forcing_ratio)\n",
        "\n",
        "            multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
        "            multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
        "            num_words = multi_step_preds.shape[0]\n",
        "\n",
        "            if ignore_padding: ## if padding has to be ignored.\n",
        "                ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
        "                ## argmax is not supported for bool on cuda, hence casting it to long.\n",
        "                padding_start = torch.argmax((target_tensor == train_loader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
        "                ## Creating a mask with 1's in each position of a padding token\n",
        "                mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
        "\n",
        "                ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
        "                tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
        "                tot_words += num_words\n",
        "\n",
        "            loss = loss_criterion(\n",
        "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "                target_tensor.view(-1)\n",
        "            )\n",
        "            loss.backward()\n",
        "            encoder_optim.step()\n",
        "            decoder_optim.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = round(epoch_loss / len(train_loader),4)\n",
        "        epoch_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
        "        return epoch_loss,epoch_accuracy\n",
        "\n",
        "    def train(self,train_loader,valid_loader, encoder, decoder, epochs,padding_idx,optimiser = \"adam\",loss=\"crossentropy\",weight_decay=0, lr=0.001,teacher_forcing_ratio = 0,device='cpu',wandb_logging = False):\n",
        "        \"\"\"\n",
        "        The method to train the encoder-decoder model.\n",
        "        \"\"\"\n",
        "        ## specify the optimiser\n",
        "        if optimiser.lower() == \"adam\":\n",
        "            encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "            decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "        elif optimiser.lower() == \"nadam\":\n",
        "            encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "            decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "        elif optimiser.lower() == \"rmsprop\":\n",
        "            encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "            decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
        "\n",
        "        ## Specify the loss criteria\n",
        "        if loss.lower() == \"crossentropy\":\n",
        "            loss_criterion = nn.CrossEntropyLoss(ignore_index = padding_idx).to(device)\n",
        "\n",
        "        lp = train_loader.dataset.lp\n",
        "\n",
        "        print(f\"Training for {epochs} epochs\")\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            ## Train for 1 epoch.\n",
        "            train_loss,train_accuracy = self.train_epoch(train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion,teacher_forcing_ratio,device=device)\n",
        "            ## compute validation accuracy.\n",
        "            val_loss,_,val_accuracy = self.compute_accuracy(valid_loader,encoder,decoder,loss_criterion,ignore_padding=True,device=device)\n",
        "            print(f\"Epoch {epoch+1}\\\\t Train Loss : {train_loss}\\\\t Train Acc : {train_accuracy}% \\\\t Val Loss : {val_loss}\\\\t Val Acc : {val_accuracy}%\")\n",
        "            if wandb_logging:\n",
        "                wandb.log({'epoch': epoch+1,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
        "\n",
        "    def compute_accuracy(self,dataloader,encoder,decoder,criterion,ignore_padding = True,device='cpu',save_results=False,filename=\"\"):\n",
        "        \"\"\"\n",
        "        Method to compute the accuracy using the model (encoder-decoder) using dataloader.\n",
        "        \"\"\"\n",
        "        char_lvl_accuracy = 0\n",
        "        word_level_accuracy = 0\n",
        "        tot_chars = 0\n",
        "        tot_words = 0\n",
        "        tot_correct_char_preds = 0\n",
        "        tot_correct_word_preds = 0\n",
        "        loss = 0\n",
        "\n",
        "        if save_results:\n",
        "            rows = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            train = 0\n",
        "            if encoder.training and decoder.training: ## reset the the model back to train mode\n",
        "                train = 1\n",
        "            encoder.eval()\n",
        "            decoder.eval()\n",
        "\n",
        "            for data in dataloader:\n",
        "                input_tensor, target_tensor,_,target_max_len = data\n",
        "                batch_size = data[0].shape[0]\n",
        "\n",
        "                if encoder.rnn_type == \"LSTM\":\n",
        "                    encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
        "                    encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
        "                else:\n",
        "                    encoder_hidden = None\n",
        "                    encoder_cell = None\n",
        "\n",
        "                encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
        "                ## even though we are passing target tensor, the teacher forcing ratio is 0, so no teacher forcing\n",
        "                decoder_outputs, _, attentions = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor = target_tensor,teacher_forcing_ratio = 0)\n",
        "                loss += criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1)).item()\n",
        "\n",
        "                ## For a batch, for each character find the most probable output word.\n",
        "                multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
        "                multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
        "                num_chars = multi_step_preds.numel() ##find the total number of characters in the current batch\n",
        "                num_words = multi_step_preds.shape[0] ##find the total number of words in the current batch.\n",
        "\n",
        "                if ignore_padding: ## if padding has to be ignored.\n",
        "                    ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
        "                    ## argmax is not supported for bool on cuda, hence casting it to long.\n",
        "                    padding_start = torch.argmax((target_tensor == dataloader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
        "                    ## Creating a mask with 1's in each position of a padding token\n",
        "                    mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
        "\n",
        "                    ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
        "                    tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
        "                    tot_words += num_words\n",
        "\n",
        "                    ##creating a complement of the mask so to mark padding tokens as irrelevant\n",
        "                    complement_mask = (1-mask.int()).bool()\n",
        "                    num_pad_chars = mask.int().sum().item()\n",
        "                    ##counting number of non_pad_chars to compute accuracy.\n",
        "                    num_non_pad_chars = num_chars - num_pad_chars\n",
        "\n",
        "                    tot_correct_char_preds += (torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()).item()\n",
        "                    tot_chars += num_non_pad_chars\n",
        "\n",
        "                    if save_results:\n",
        "                        word_preds_correctness = torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int()\n",
        "                        for i in range(multi_step_preds.shape[0]):\n",
        "                            rows.append([dataloader.dataset.lp.decode_word(input_tensor[i].cpu().numpy(),lang_id=0),dataloader.dataset.lp.decode_word(target_tensor[i].cpu().numpy(),lang_id=1),dataloader.dataset.lp.decode_word(multi_step_preds[i].cpu().numpy(),lang_id=1),word_preds_correctness[i].cpu().item()])\n",
        "                else: ##otherwise.\n",
        "                    tot_correct_word_preds += (torch.all(multi_step_pred_correctness,dim=1).int().sum()).item()\n",
        "                    tot_words += num_words\n",
        "                    tot_correct_char_preds += (multi_step_pred_correctness.int().sum()).item()\n",
        "                    tot_chars += num_chars\n",
        "\n",
        "            char_lvl_accuracy = round(tot_correct_char_preds*100/tot_chars,2)\n",
        "            word_lvl_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
        "            loss /= dataloader.dataset.data.shape[0]\n",
        "\n",
        "            if save_results:\n",
        "                df = pd.DataFrame(data=rows, columns=[\"Source Word\",\"Target Word\",\"Predicted Word\",\"Is Prediction Correct\"])\n",
        "                df.to_csv(filename+\".csv\",index=False)\n",
        "\n",
        "            if train:\n",
        "                encoder.train()\n",
        "                decoder.train()\n",
        "\n",
        "            return round(loss,4),char_lvl_accuracy,word_lvl_accuracy\n",
        "''')\n",
        "\n",
        "# Write visualize_attention.py\n",
        "with open('visualize_attention.py', 'w') as f:\n",
        "    f.write('''\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "from Core_Utils import *\n",
        "from Encoder_Decoder_Architecture import *\n",
        "from Machine_Translator import *\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def visualize_attention(model, input_word, target_word, input_lang, target_lang, pad_token_id, device, ax=None):\n",
        "    \"\"\"\n",
        "    Visualizes attention weights for a given input and target word pair.\n",
        "    \"\"\"\n",
        "    # Create figure if not provided\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Encode input word\n",
        "    input_seq = input_lang.encode_word(input_word, input_lang.source_lang, padding=False, append_eos=True)\n",
        "    input_tensor = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.encoder.eval()\n",
        "    model.decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_size = 1\n",
        "\n",
        "        if model.encoder.rnn_type == \"LSTM\":\n",
        "            encoder_hidden = torch.zeros(model.encoder.num_layers*model.encoder.D, batch_size,\n",
        "                                         model.encoder.hidden_size, device=device)\n",
        "            encoder_cell = torch.zeros(model.encoder.num_layers*model.encoder.D, batch_size,\n",
        "                                        model.encoder.hidden_size, device=device)\n",
        "        else:\n",
        "            encoder_hidden = None\n",
        "            encoder_cell = None\n",
        "\n",
        "        # Run encoder\n",
        "        encoder_outputs, encoder_hidden, encoder_cell = model.encoder(input_tensor, encoder_hidden, encoder_cell)\n",
        "\n",
        "        # Run decoder with teacher forcing disabled\n",
        "        decoder_outputs, _, attention_weights = model.decoder(\n",
        "            encoder_outputs, encoder_hidden, encoder_cell, target_tensor=None,\n",
        "            eval_mode=True, teacher_forcing_ratio=0\n",
        "        )\n",
        "\n",
        "        # Get predicted output\n",
        "        _, predicted_indices = torch.max(decoder_outputs, dim=2)\n",
        "        predicted_word = target_lang.decode_word(predicted_indices[0].cpu().numpy(), target_lang.target_lang)\n",
        "        predicted_word = ''.join(predicted_word)\n",
        "\n",
        "        # Concatenate attention weights\n",
        "        attention = torch.cat([a for a in attention_weights], 0).squeeze(1).cpu().numpy()\n",
        "\n",
        "    # Get input characters (remove <end> token)\n",
        "    input_chars = [input_lang.source_id2char[i] for i in input_seq if i != pad_token_id and i != input_lang.end_token_id]\n",
        "\n",
        "    # Get output characters from prediction\n",
        "    output_chars = list(predicted_word)\n",
        "\n",
        "    # Trim attention matrix to actual sequence length\n",
        "    attention = attention[:len(output_chars), :len(input_chars)]\n",
        "\n",
        "    # Create heatmap\n",
        "    cax = ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    # Set labels\n",
        "    ax.set_xticklabels([''] + input_chars, rotation=90)\n",
        "    ax.set_yticklabels([''] + output_chars)\n",
        "\n",
        "    # Major ticks\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    # Add title with input, prediction and target\n",
        "    ax.set_title(f'Input: {input_word}\\\\nPrediction: {predicted_word}\\\\nTarget: {target_word}')\n",
        "\n",
        "    return ax\n",
        "\n",
        "def visualize_examples(model, test_loader, num_examples=9):\n",
        "    \"\"\"\n",
        "    Visualize attention weights for multiple examples\n",
        "    \"\"\"\n",
        "    # Get a batch of examples\n",
        "    batch = next(iter(test_loader))\n",
        "    input_tensors, target_tensors = batch[0], batch[1]\n",
        "\n",
        "    # Create a grid of figures\n",
        "    rows = int(np.ceil(num_examples / 3))\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(18, 6*rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    lp = test_loader.dataset.lp\n",
        "    pad_token_id = lp.pad_token_id\n",
        "\n",
        "    for i in range(min(num_examples, len(input_tensors))):\n",
        "        # Get input and target words\n",
        "        input_seq = input_tensors[i].cpu().numpy()\n",
        "        target_seq = target_tensors[i].cpu().numpy()\n",
        "\n",
        "        input_word = ''.join(lp.decode_word(input_seq, lp.source_lang))\n",
        "        target_word = ''.join(lp.decode_word(target_seq, lp.target_lang))\n",
        "\n",
        "        # Visualize attention for this example\n",
        "        visualize_attention(model, input_word, target_word, lp, lp, pad_token_id, model.device, ax=axes[i])\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for i in range(num_examples, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('attention_heatmaps.png')\n",
        "    plt.show()\n",
        "    return fig\n",
        "''')\n",
        "\n",
        "# Write trainer.py\n",
        "with open('trainer.py', 'w') as f:\n",
        "    f.write('''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "from functools import partial\n",
        "import argparse\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "from Core_Utils import *\n",
        "from Encoder_Decoder_Architecture import *\n",
        "from Machine_Translator import *\n",
        "\n",
        "seed = 23\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def train_model(config, model_type=\"vanilla\", wandb_log=False):\n",
        "    \"\"\"\n",
        "    Main function to train a seq2seq model.\n",
        "\n",
        "    Args:\n",
        "        config: Dictionary with model hyperparameters\n",
        "        model_type: \"vanilla\" or \"attention\"\n",
        "        wandb_log: Whether to log metrics to wandb\n",
        "\n",
        "    Returns:\n",
        "        Trained model\n",
        "    \"\"\"\n",
        "    batch_size = config['batch_size']\n",
        "    target_lang = \"tel\"\n",
        "    base_dir = \"data_folder/\"\n",
        "\n",
        "    # Setup device\n",
        "    if config['device'] == None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    else:\n",
        "        device = torch.device(config['device'])\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Setup data\n",
        "    use_meta_tokens = True\n",
        "    lang_dir = base_dir + target_lang + \"/\"\n",
        "\n",
        "    # Create train loader\n",
        "    train_lp = LanguageProcessor(language_directory=lang_dir, target_lang_name=target_lang, mode=\"train\", meta_tokens=use_meta_tokens)\n",
        "    pad_token_id = train_lp.source_char2id['<pad>']\n",
        "\n",
        "    collate_fn_ptr = partial(collate_fn, pad_token_id=pad_token_id, device=device)\n",
        "\n",
        "    train_dataset = WordDataset(train_lp, device=device)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn_ptr, shuffle=True)\n",
        "\n",
        "    # Create test loader\n",
        "    test_lp = LanguageProcessor(language_directory=lang_dir, target_lang_name=target_lang, mode=\"test\", meta_tokens=use_meta_tokens)\n",
        "    test_lp.source_char2id = train_lp.source_char2id\n",
        "    test_lp.source_id2char = train_lp.source_id2char\n",
        "    test_lp.target_char2id = train_lp.target_char2id\n",
        "    test_lp.target_id2char = train_lp.target_id2char\n",
        "\n",
        "    test_dataset = WordDataset(test_lp, device=device)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn_ptr, shuffle=True)\n",
        "\n",
        "    # Create validation loader\n",
        "    valid_lp = LanguageProcessor(language_directory=lang_dir, target_lang_name=target_lang, mode=\"valid\", meta_tokens=use_meta_tokens)\n",
        "    valid_lp.source_char2id = train_lp.source_char2id\n",
        "    valid_lp.source_id2char = train_lp.source_id2char\n",
        "    valid_lp.target_char2id = train_lp.target_char2id\n",
        "    valid_lp.target_id2char = train_lp.target_id2char\n",
        "\n",
        "    valid_dataset = WordDataset(valid_lp, device=device)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn_ptr, shuffle=True)\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    source_vocab_size = len(list(train_lp.source_char2id.keys()))\n",
        "    target_vocab_size = len(list(train_lp.target_char2id.keys()))\n",
        "\n",
        "    # Extract hyperparameters\n",
        "    hidden_size = config['hidden_size']\n",
        "    embedding_size = hidden_size\n",
        "    epochs = config['epochs']\n",
        "    optimiser = config['optimiser']\n",
        "    weight_decay = config['weight_decay']\n",
        "    lr = config['lr']\n",
        "    num_layers = config['num_layers']\n",
        "    rnn_type = config['rnn_type'].upper()\n",
        "    bidirectional = config['bidirectional']\n",
        "    teacher_forcing_ratio = config['teacher_forcing_ratio']\n",
        "    dropout = config['dropout']\n",
        "\n",
        "    # Set use_attention based on model type\n",
        "    use_attention = True if model_type == \"attention\" else False\n",
        "\n",
        "    # Create model\n",
        "    model = MachineTranslator(\n",
        "        source_vocab_size=source_vocab_size,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        hidden_size=hidden_size,\n",
        "        embedding_size=embedding_size,\n",
        "        rnn_type=rnn_type,\n",
        "        batch_size=batch_size,\n",
        "        pad_token_id=pad_token_id,\n",
        "        dropout=dropout,\n",
        "        num_layers=num_layers,\n",
        "        bidirectional=bidirectional,\n",
        "        use_attention=use_attention,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    model.train(\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        encoder=model.encoder,\n",
        "        decoder=model.decoder,\n",
        "        epochs=epochs,\n",
        "        padding_idx=pad_token_id,\n",
        "        optimiser=optimiser,\n",
        "        weight_decay=weight_decay,\n",
        "        lr=lr,\n",
        "        teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "        device=device,\n",
        "        wandb_logging=wandb_log\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    loss_criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id).to(device)\n",
        "    test_loss, char_acc, test_accuracy = model.compute_accuracy(\n",
        "        test_loader,\n",
        "        model.encoder,\n",
        "        model.decoder,\n",
        "        loss_criterion,\n",
        "        ignore_padding=True,\n",
        "        device=device,\n",
        "        save_results=True,\n",
        "        filename=f\"predictions_{model_type}\"\n",
        "    )\n",
        "    print(f\"Testing Loss: {test_loss}\\\\tChar Accuracy: {char_acc}%\\\\tWord Accuracy: {test_accuracy}%\")\n",
        "\n",
        "    if wandb_log:\n",
        "        # Get the WandB run information\n",
        "        entity_name = wandb.run.entity\n",
        "        project_name = wandb.run.project\n",
        "        run_id = wandb.run.id\n",
        "\n",
        "        # Print direct links to WandB dashboards\n",
        "        print(\"\\\\n=== WandB Dashboard Links ===\")\n",
        "        print(f\"Project dashboard: https://wandb.ai/{entity_name}/{project_name}\")\n",
        "        print(f\"This run: https://wandb.ai/{entity_name}/{project_name}/runs/{run_id}\")\n",
        "        print(f\"Compare all runs: https://wandb.ai/{entity_name}/{project_name}/table\")\n",
        "\n",
        "        # For Google Colab, we can create a clickable HTML link\n",
        "        from IPython.display import display, HTML\n",
        "        display(HTML(f'<a href=\"https://wandb.ai/{entity_name}/{project_name}\" target=\"_blank\">Open WandB Dashboard</a>'))\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Model type\n",
        "    parser.add_argument(\"--model\", type=str, default=\"vanilla\", choices=[\"vanilla\", \"attention\"],\n",
        "                       help=\"Model type: vanilla seq2seq or seq2seq with attention\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64,\n",
        "                       help=\"Batch size used to train neural network.\")\n",
        "    parser.add_argument(\"-bid\", \"--bidirectional\", type=bool, default=True, choices=[True, False],\n",
        "                       help=\"If True, input would be seen in both directions.\")\n",
        "    parser.add_argument(\"-dpt\", \"--dropout\", type=float, default=0.2,\n",
        "                       help=\"The dropout probability.\")\n",
        "    parser.add_argument(\"-es\", \"--embedding_size\", type=int, default=256,\n",
        "                       help=\"The input embedding dimension.\")\n",
        "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=15,\n",
        "                       help=\"Number of epochs to train.\")\n",
        "    parser.add_argument(\"-hs\", \"--hidden_size\", type=int, default=256,\n",
        "                       help=\"The dimension of the hidden state.\")\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=3e-4,\n",
        "                       help=\"Learning rate used to optimize model parameters.\")\n",
        "    parser.add_argument(\"-nl\", \"--num_layers\", type=int, default=2,\n",
        "                       help=\"Number of Recurrence Layers.\")\n",
        "    parser.add_argument(\"-o\", \"--optimizer\", type=str, default=\"nadam\", choices=[\"rmsprop\", \"adam\", \"nadam\"],\n",
        "                       help=\"Optimizer used to minimize the loss.\")\n",
        "    parser.add_argument(\"-rt\", \"--rnn_type\", type=str, default=\"lstm\", choices=[\"lstm\", \"gru\", \"rnn\"],\n",
        "                       help=\"The type of recurrent cell to be used.\")\n",
        "    parser.add_argument(\"-tf\", \"--teacher_forcing\", type=float, default=0.4,\n",
        "                       help=\"The Teacher Forcing Ratio.\")\n",
        "    parser.add_argument(\"-w_d\", \"--weight_decay\", type=float, default=1e-5,\n",
        "                       help=\"Weight decay used by optimizers.\")\n",
        "    parser.add_argument(\"-d\", \"--device\", type=str, default=None,\n",
        "                       help=\"The device on which the training happens.\")\n",
        "    parser.add_argument(\"--wandb\", action=\"store_true\",\n",
        "                       help=\"Log metrics to Weights & Biases\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config = {\n",
        "        'batch_size': args.batch_size,\n",
        "        'bidirectional': args.bidirectional,\n",
        "        'dropout': args.dropout,\n",
        "        'embedding_size': args.embedding_size,\n",
        "        'epochs': args.epochs,\n",
        "        'hidden_size': args.hidden_size,\n",
        "        'lr': args.learning_rate,\n",
        "        'num_layers': args.num_layers,\n",
        "        'optimiser': args.optimizer,\n",
        "        'rnn_type': args.rnn_type,\n",
        "        'teacher_forcing_ratio': args.teacher_forcing,\n",
        "        'weight_decay': args.weight_decay,\n",
        "        'device': args.device\n",
        "    }\n",
        "\n",
        "    if args.wandb:\n",
        "        # Initialize wandb\n",
        "        wandb.init(\n",
        "            project=f\"cs6910_assignment3_{args.model}\",\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "    model = train_model(config, model_type=args.model, wandb_log=args.wandb)\n",
        "''')\n",
        "\n",
        "# Create data directory\n",
        "!mkdir -p data_folder/tel\n",
        "\n",
        "# Copy data files (adjust paths to match your Google Drive data location)\n",
        "data_base_path = \"/content/drive/MyDrive/dakshina_dataset_v1.0/te/lexicons/\"\n",
        "!cp {data_base_path}te.translit.sampled.train.tsv data_folder/tel/train.txt\n",
        "!cp {data_base_path}te.translit.sampled.dev.tsv data_folder/tel/valid.txt\n",
        "!cp {data_base_path}te.translit.sampled.test.tsv data_folder/tel/test.txt\n",
        "\n",
        "print(\"Setup complete! Files created:\")\n",
        "!ls -la\n",
        "\n",
        "# To train a model, uncomment one of these lines:\n",
        "!python trainer.py --model vanilla -b 64 -bid True -dpt 0.2 -es 128 -e 15 -hs 256 -nl 2 -o nadam -rt lstm -tf 0.4 -w_d 1e-5 --wandb\n",
        "# !python trainer.py --model attention -b 64 -bid True -dpt 0.0 -es 256 -e 15 -hs 512 -nl 2 -o rmsprop -rt gru -tf 0.4 -w_d 1e-5 --wandb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPtrJTkDzr78f0tXbBOx2a2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}