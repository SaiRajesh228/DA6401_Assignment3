{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVHdoTR6/7e/bvfqyyHEUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiRajesh228/DA6401_Assignment3/blob/main/withAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-Z4Hacp5xlU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import csv\n",
        "import wandb\n",
        "\n",
        "# Log in to Weights & Biases with the provided key\n",
        "wandb.login(key='32f6049439fd96afecb91b2853dcb24d77f2f9d3')\n",
        "\n",
        "# For reproducibility\n",
        "def set_random_seeds(seed=42):\n",
        "    \"\"\"Set random seeds for reproducibility across libraries\"\"\"\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Character vocabulary class\n",
        "class CharacterVocabulary:\n",
        "    \"\"\"Character-level vocabulary for transliteration tasks\"\"\"\n",
        "    def __init__(self, token_list=None, special_tokens=['<pad>','<bos>','<eos>','<unk>']):\n",
        "        self.special_tokens = special_tokens\n",
        "        self.idx_to_char = list(special_tokens) + (token_list or [])\n",
        "        self.char_to_idx = {ch:i for i,ch in enumerate(self.idx_to_char)}\n",
        "\n",
        "    @classmethod\n",
        "    def create_from_texts(cls, text_list):\n",
        "        \"\"\"Build vocabulary from a list of text samples\"\"\"\n",
        "        unique_chars = sorted({char for text in text_list for char in text})\n",
        "        return cls(token_list=unique_chars)\n",
        "\n",
        "    @classmethod\n",
        "    def create_from_file(cls, file_path, src_col='src', tgt_col='tgt', is_csv=True):\n",
        "        \"\"\"Build vocabulary from a data file (CSV or TSV)\"\"\"\n",
        "        if is_csv:\n",
        "            df = pd.read_csv(file_path, header=None, names=[src_col, tgt_col])\n",
        "            texts = df[src_col].dropna().tolist() + df[tgt_col].dropna().tolist()\n",
        "        else:\n",
        "            texts = []\n",
        "            with open(file_path, encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        texts.extend([parts[0], parts[1]])\n",
        "\n",
        "        return cls.create_from_texts(texts)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Save vocabulary to JSON file\"\"\"\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.idx_to_char, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\"Load vocabulary from JSON file\"\"\"\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            idx_to_char = json.load(f)\n",
        "\n",
        "        vocab = cls(token_list=[])\n",
        "        vocab.idx_to_char = idx_to_char\n",
        "        vocab.char_to_idx = {c:i for i,c in enumerate(idx_to_char)}\n",
        "        return vocab\n",
        "\n",
        "    def tokenize(self, text, add_bos=False, add_eos=False):\n",
        "        \"\"\"Convert text to a sequence of indices\"\"\"\n",
        "        indices = []\n",
        "        if add_bos: indices.append(self.char_to_idx['<bos>'])\n",
        "        for c in text:\n",
        "            indices.append(self.char_to_idx.get(c, self.char_to_idx['<unk>']))\n",
        "        if add_eos: indices.append(self.char_to_idx['<eos>'])\n",
        "        return indices\n",
        "\n",
        "    def detokenize(self, indices, remove_special=True, join=True):\n",
        "        \"\"\"Convert a sequence of indices back to text\"\"\"\n",
        "        if hasattr(indices, 'tolist'):\n",
        "            indices = indices.tolist()\n",
        "\n",
        "        chars = [self.idx_to_char[i] for i in indices if i < len(self.idx_to_char)]\n",
        "\n",
        "        if remove_special:\n",
        "            chars = [c for c in chars if c not in self.special_tokens]\n",
        "\n",
        "        return ''.join(chars) if join else chars\n",
        "\n",
        "    def batch_detokenize(self, batch_indices, remove_special=True):\n",
        "        \"\"\"Decode a batch of index sequences\"\"\"\n",
        "        return [self.detokenize(seq, remove_special=remove_special) for seq in batch_indices]\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get vocabulary statistics\"\"\"\n",
        "        return {\n",
        "            'total_size': len(self.idx_to_char),\n",
        "            'special_tokens': len(self.special_tokens),\n",
        "            'character_count': len(self.idx_to_char) - len(self.special_tokens)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_char)\n",
        "\n",
        "    @property\n",
        "    def pad_id(self): return self.char_to_idx['<pad>']\n",
        "\n",
        "    @property\n",
        "    def bos_id(self): return self.char_to_idx['<bos>']\n",
        "\n",
        "    @property\n",
        "    def eos_id(self): return self.char_to_idx['<eos>']\n",
        "\n",
        "    @property\n",
        "    def unk_id(self): return self.char_to_idx['<unk>']\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self): return len(self.idx_to_char)\n",
        "\n",
        "# Data processing\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"Dataset class for transliteration tasks\"\"\"\n",
        "\n",
        "    def __init__(self, file_path, source_vocab, target_vocab, dataset_type='dakshina'):\n",
        "        self.examples = []\n",
        "        self.dataset_type = dataset_type\n",
        "\n",
        "        if dataset_type == 'dakshina':\n",
        "            for src, tgt in self._read_tsv_file(file_path):\n",
        "                src_ids = source_vocab.tokenize(src, add_bos=True, add_eos=True)\n",
        "                tgt_ids = target_vocab.tokenize(tgt, add_bos=True, add_eos=True)\n",
        "                self.examples.append((\n",
        "                    torch.tensor(src_ids, dtype=torch.long),\n",
        "                    torch.tensor(tgt_ids, dtype=torch.long)\n",
        "                ))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset type: {dataset_type}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "    def _read_tsv_file(self, path):\n",
        "        \"\"\"Read a tab-separated file with source and target text\"\"\"\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    yield parts[1], parts[0]  # Dakshina format: target, source\n",
        "\n",
        "def create_batches(batch, src_vocab, tgt_vocab):\n",
        "    \"\"\"Custom collate function for variable-length sequences\"\"\"\n",
        "    srcs, tgts = zip(*batch)\n",
        "    src_padded = pad_sequence(srcs, batch_first=True, padding_value=src_vocab.pad_id)\n",
        "    tgt_padded = pad_sequence(tgts, batch_first=True, padding_value=tgt_vocab.pad_id)\n",
        "    src_lengths = torch.tensor([len(s) for s in srcs], dtype=torch.long)\n",
        "    return src_padded, src_lengths, tgt_padded\n",
        "\n",
        "def load_data(\n",
        "        language='te',\n",
        "        dataset_type='dakshina',\n",
        "        dataset_path=None,\n",
        "        batch_size=64,\n",
        "        device='cpu',\n",
        "        worker_count=2,\n",
        "        prefetch_factor=4,\n",
        "        persistent_workers=True,\n",
        "        cache_dir='./cache',\n",
        "        use_cached_vocab=True\n",
        "    ):\n",
        "    \"\"\"Load transliteration datasets and vocabulary\"\"\"\n",
        "    if dataset_path is None:\n",
        "        dataset_path = os.path.join(\n",
        "            '/content/dakshina_dataset_v1.0',\n",
        "            language, 'lexicons'\n",
        "        )\n",
        "\n",
        "    # Create cache directory if it doesn't exist\n",
        "    if use_cached_vocab:\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        vocab_cache_path = os.path.join(cache_dir, f\"{language}_{dataset_type}_vocab.pkl\")\n",
        "\n",
        "    # Try to load cached vocabularies\n",
        "    if use_cached_vocab and os.path.exists(vocab_cache_path):\n",
        "        print(f\"Loading cached vocabularies from {vocab_cache_path}\")\n",
        "        with open(vocab_cache_path, 'rb') as f:\n",
        "            src_vocab, tgt_vocab = pickle.load(f)\n",
        "    else:\n",
        "        # Build vocabularies from data\n",
        "        all_src, all_tgt = [], []\n",
        "\n",
        "        for split in ['train', 'dev']:\n",
        "            file_path = os.path.join(dataset_path, f\"{language}.translit.sampled.{split}.tsv\")\n",
        "            with open(file_path, encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) >= 2:\n",
        "                        all_src.append(parts[1])  # Dakshina format has target, source\n",
        "                        all_tgt.append(parts[0])\n",
        "\n",
        "        # Build vocabularies\n",
        "        src_vocab = CharacterVocabulary.create_from_texts(all_src)\n",
        "        tgt_vocab = CharacterVocabulary.create_from_texts(all_tgt)\n",
        "\n",
        "        # Cache vocabularies\n",
        "        if use_cached_vocab:\n",
        "            with open(vocab_cache_path, 'wb') as f:\n",
        "                pickle.dump((src_vocab, tgt_vocab), f)\n",
        "\n",
        "    # DataLoader configuration\n",
        "    loader_config = dict(\n",
        "        batch_size=batch_size,\n",
        "        num_workers=worker_count,\n",
        "        prefetch_factor=prefetch_factor,\n",
        "        persistent_workers=persistent_workers and worker_count > 0,\n",
        "        pin_memory=(device == 'cuda')\n",
        "    )\n",
        "\n",
        "    # Create data loaders for each split\n",
        "    data_loaders = {}\n",
        "\n",
        "    splits = {'train': 'train', 'dev': 'dev', 'test': 'test'}\n",
        "    for split_name, file_split in splits.items():\n",
        "        file_path = os.path.join(dataset_path, f\"{language}.translit.sampled.{file_split}.tsv\")\n",
        "        dataset = TransliterationDataset(file_path, src_vocab, tgt_vocab, dataset_type='dakshina')\n",
        "        data_loaders[split_name] = DataLoader(\n",
        "            dataset,\n",
        "            shuffle=(split_name == 'train'),\n",
        "            collate_fn=lambda b: create_batches(b, src_vocab, tgt_vocab),\n",
        "            **loader_config\n",
        "        )\n",
        "\n",
        "    return data_loaders, src_vocab, tgt_vocab\n",
        "\n",
        "# Model Components\n",
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1,\n",
        "                 rnn_type='LSTM', dropout=0.0, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Output size will be doubled if bidirectional\n",
        "        self.output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "\n",
        "        rnn_classes = {'LSTM': nn.LSTM, 'GRU': nn.GRU, 'RNN': nn.RNN}\n",
        "        if rnn_type not in rnn_classes:\n",
        "            raise ValueError(f\"Unsupported RNN type: {rnn_type}\")\n",
        "\n",
        "        self.rnn = rnn_classes[rnn_type](embedding_dim,\n",
        "                                       hidden_dim,\n",
        "                                       num_layers=num_layers,\n",
        "                                       dropout=dropout if num_layers > 1 else 0.0,\n",
        "                                       batch_first=True,\n",
        "                                       bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        # inputs: [batch_size, seq_len], lengths: [batch_size]\n",
        "        embedded = self.embedding(inputs)  # [batch_size, seq_len, embedding_dim]\n",
        "        packed_input = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden_states = self.rnn(packed_input)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)  # [batch_size, seq_len, hidden_dim*dirs]\n",
        "\n",
        "        # Process hidden state based on RNN type and bidirectionality\n",
        "        if self.bidirectional:\n",
        "            if self.rnn_type == 'LSTM':\n",
        "                # For LSTM we have both hidden and cell states\n",
        "                h_n, c_n = hidden_states\n",
        "                # Combine forward and backward states by averaging\n",
        "                h_n = torch.add(h_n[0:self.num_layers], h_n[self.num_layers:]) / 2\n",
        "                c_n = torch.add(c_n[0:self.num_layers], c_n[self.num_layers:]) / 2\n",
        "                hidden_states = (h_n, c_n)\n",
        "            else:\n",
        "                # For GRU/RNN we only have hidden state\n",
        "                hidden_states = torch.add(hidden_states[0:self.num_layers], hidden_states[self.num_layers:]) / 2\n",
        "\n",
        "        return output, hidden_states\n",
        "\n",
        "# Bahdanau Attention mechanism - the core of the attention model\n",
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Bahdanau attention mechanism (additive attention)\n",
        "\n",
        "    This attention mechanism allows the decoder to focus on different parts\n",
        "    of the encoder's outputs at each decoding step, enabling the model to\n",
        "    better capture alignments between source and target sequences.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_dim, decoder_dim):\n",
        "        super().__init__()\n",
        "        # Linear layer to process concatenated encoder and decoder states\n",
        "        self.attention_layer = nn.Linear(encoder_dim + decoder_dim, decoder_dim)\n",
        "        # Vector to convert processed states to attention scores\n",
        "        self.v_layer = nn.Linear(decoder_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        \"\"\"\n",
        "        Calculate attention weights\n",
        "\n",
        "        Args:\n",
        "            hidden: [batch_size, decoder_dim] - Current decoder hidden state\n",
        "            encoder_outputs: [batch_size, src_len, encoder_dim] - All encoder outputs\n",
        "            mask: [batch_size, src_len] - Source padding mask (1 for real tokens, 0 for padding)\n",
        "\n",
        "        Returns:\n",
        "            attention_weights: [batch_size, src_len] - Attention weights for each position\n",
        "        \"\"\"\n",
        "        batch_size, src_len, _ = encoder_outputs.size()\n",
        "\n",
        "        # Repeat decoder hidden state for each encoder position\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, decoder_dim]\n",
        "\n",
        "        # Attention energy calculation\n",
        "        # Concatenate encoder and decoder representations\n",
        "        combined = torch.cat((hidden, encoder_outputs), dim=2)  # [batch_size, src_len, encoder_dim+decoder_dim]\n",
        "\n",
        "        # Process through attention layer and apply tanh activation\n",
        "        energy = torch.tanh(self.attention_layer(combined))  # [batch_size, src_len, decoder_dim]\n",
        "\n",
        "        # Convert to attention scores\n",
        "        attention_scores = self.v_layer(energy).squeeze(2)  # [batch_size, src_len]\n",
        "\n",
        "        # Apply mask to ignore padding positions - set scores at pad positions to negative infinity\n",
        "        attention_scores = attention_scores.masked_fill(~mask, -1e10)\n",
        "\n",
        "        # Apply softmax to get attention weights that sum to 1\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # [batch_size, src_len]\n",
        "\n",
        "        return attention_weights\n",
        "\n",
        "class AttentionDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    RNN decoder with Bahdanau attention mechanism\n",
        "\n",
        "    This decoder attends to the encoder outputs at each decoding step, allowing\n",
        "    it to focus on relevant parts of the input sequence when generating the output.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_hidden_dim, decoder_hidden_dim,\n",
        "                 num_layers=1, rnn_type=\"LSTM\", dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        # Initialize the attention mechanism\n",
        "        self.attention = BahdanauAttention(encoder_hidden_dim, decoder_hidden_dim)\n",
        "\n",
        "        # Input to RNN is embedding + context vector from attention\n",
        "        rnn_input_dim = embedding_dim + encoder_hidden_dim\n",
        "\n",
        "        # Input to output layer combines RNN hidden state, context vector from attention, and embedding\n",
        "        # This concatenation is important for the model to have direct access to both\n",
        "        # the current context and the embedding of the previous token\n",
        "        fc_input_dim = decoder_hidden_dim + encoder_hidden_dim + embedding_dim\n",
        "\n",
        "        # Select the appropriate RNN type\n",
        "        rnn_classes = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU, \"RNN\": nn.RNN}\n",
        "        if rnn_type not in rnn_classes:\n",
        "            raise ValueError(f\"Unsupported RNN type: {rnn_type}\")\n",
        "\n",
        "        self.rnn = rnn_classes[rnn_type](rnn_input_dim, decoder_hidden_dim,\n",
        "                                        num_layers=num_layers,\n",
        "                                        dropout=dropout if num_layers > 1 else 0.0,\n",
        "                                        batch_first=True)\n",
        "\n",
        "        # Final output layer that predicts the next token\n",
        "        self.output_layer = nn.Linear(fc_input_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_token, hidden, encoder_outputs, mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the attention decoder\n",
        "\n",
        "        Args:\n",
        "            input_token : [batch_size] - Current input token\n",
        "            hidden : tuple(tensor) or tensor - Initial RNN state\n",
        "            encoder_outputs : [batch_size, src_len, encoder_hidden_dim] - Encoder outputs\n",
        "            mask : [batch_size, src_len] - Source padding mask\n",
        "\n",
        "        Returns:\n",
        "            logits : [batch_size, vocab_size] - Prediction logits\n",
        "            hidden : Updated RNN state\n",
        "            attention_weights : [batch_size, src_len] - Attention weights\n",
        "        \"\"\"\n",
        "        # Embed the current token\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # [batch_size, 1, embedding_dim]\n",
        "\n",
        "        # Extract the hidden state for attention (depends on RNN type)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # For LSTM, use the hidden state (not cell state)\n",
        "            attn_hidden = hidden[0][-1]\n",
        "        else:\n",
        "            # For GRU/RNN\n",
        "            attn_hidden = hidden[-1]\n",
        "\n",
        "        # Calculate attention weights over encoder outputs\n",
        "        attention_weights = self.attention(attn_hidden, encoder_outputs, mask)  # [batch_size, src_len]\n",
        "\n",
        "        # Create context vector by applying attention weights to encoder outputs\n",
        "        # This is a key step: we use the attention weights to create a weighted sum\n",
        "        # of encoder outputs, focusing on relevant parts of the input sequence\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, encoder_hidden_dim]\n",
        "\n",
        "        # Combine embedding and context as input to the RNN\n",
        "        # By concatenating these, the RNN can use both the previous token and the relevant\n",
        "        # context from the encoder when generating the next token\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, embedding_dim + encoder_hidden_dim]\n",
        "\n",
        "        # Run through RNN\n",
        "        output, hidden = self.rnn(rnn_input, hidden)  # [batch_size, 1, decoder_hidden_dim]\n",
        "        output = output.squeeze(1)  # [batch_size, decoder_hidden_dim]\n",
        "        embedded = embedded.squeeze(1)  # [batch_size, embedding_dim]\n",
        "        context = context.squeeze(1)  # [batch_size, encoder_hidden_dim]\n",
        "\n",
        "        # Generate prediction logits by combining all available information\n",
        "        # This allows the model to consider the RNN state, context vector, and current input token\n",
        "        logits = self.output_layer(torch.cat((output, context, embedded), dim=1))\n",
        "\n",
        "        return logits, hidden, attention_weights\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequence-to-sequence model with attention mechanism\n",
        "\n",
        "    This model consists of an encoder that processes the input sequence and\n",
        "    an attention decoder that generates the output sequence while attending\n",
        "    to relevant parts of the input.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, pad_idx, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass with teacher forcing\n",
        "\n",
        "        Args:\n",
        "            src : [batch_size, src_len] - Source sequence\n",
        "            src_lengths : [batch_size] - Lengths of each source sequence\n",
        "            tgt : [batch_size, tgt_len] - Target sequence\n",
        "            teacher_forcing_ratio : float - Probability of using teacher forcing\n",
        "\n",
        "        Returns:\n",
        "            outputs : [batch_size, tgt_len-1, vocab_size] - Decoder outputs\n",
        "        \"\"\"\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        # Create mask for attention (1 for real tokens, 0 for padding)\n",
        "        mask = (src != self.pad_idx)\n",
        "\n",
        "        batch_size, target_length = tgt.size()\n",
        "        output_dim = self.decoder.output_layer.out_features\n",
        "\n",
        "        # Initialize tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, target_length-1, output_dim, device=self.device)\n",
        "\n",
        "        # First input to the decoder is the <bos> token\n",
        "        decoder_input = tgt[:, 0]\n",
        "\n",
        "        # Teacher forcing is applied with probability teacher_forcing_ratio\n",
        "        for t in range(1, target_length):\n",
        "            # Pass through decoder with attention\n",
        "            decoder_output, hidden, _ = self.decoder(decoder_input, hidden, encoder_outputs, mask)\n",
        "\n",
        "            # Store the output\n",
        "            outputs[:, t-1] = decoder_output\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            if use_teacher_forcing:\n",
        "                # Teacher forcing: use ground-truth as next input\n",
        "                decoder_input = tgt[:, t]\n",
        "            else:\n",
        "                # No teacher forcing: use model's prediction as next input\n",
        "                decoder_input = decoder_output.argmax(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, src, src_lengths, tgt_vocab, max_len=50):\n",
        "        \"\"\"\n",
        "        Generate a translation using greedy decoding\n",
        "\n",
        "        Args:\n",
        "            src : [batch_size, src_len] - Source sequence\n",
        "            src_lengths : [batch_size] - Lengths of each source sequence\n",
        "            tgt_vocab : CharacterVocabulary - Target vocabulary\n",
        "            max_len : int - Maximum length of generated sequence\n",
        "\n",
        "        Returns:\n",
        "            generated_tokens : [batch_size, seq_len] - Generated sequences\n",
        "        \"\"\"\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        # Create mask for attention\n",
        "        mask = (src != self.pad_idx)\n",
        "\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        # First input is the <bos> token\n",
        "        decoder_input = torch.full((batch_size,), tgt_vocab.bos_id, device=self.device, dtype=torch.long)\n",
        "\n",
        "        # List to store generated tokens and attention weights\n",
        "        generated_tokens = []\n",
        "        attention_weights_list = []\n",
        "\n",
        "        # Generate tokens one by one\n",
        "        for _ in range(max_len):\n",
        "            # Get decoder output and attention weights\n",
        "            decoder_output, hidden, attn_weights = self.decoder(decoder_input, hidden, encoder_outputs, mask)\n",
        "\n",
        "            # Get the most likely token\n",
        "            next_token = decoder_output.argmax(1)\n",
        "\n",
        "            # Add to our generated tokens and attention weights\n",
        "            generated_tokens.append(next_token.unsqueeze(1))\n",
        "            attention_weights_list.append(attn_weights.unsqueeze(1))\n",
        "\n",
        "            # Update the decoder input for the next step\n",
        "            decoder_input = next_token\n",
        "\n",
        "            # Stop if all sequences have generated the <eos> token\n",
        "            if (next_token == tgt_vocab.eos_id).all():\n",
        "                break\n",
        "\n",
        "        # Concatenate all tokens and attention weights\n",
        "        return torch.cat(generated_tokens, dim=1)\n",
        "\n",
        "# Training and evaluation utilities\n",
        "def calculate_accuracy(model, data_loader, tgt_vocab, src_vocab, device):\n",
        "    \"\"\"Calculate accuracy and collect prediction details\"\"\"\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "\n",
        "    # Lists to store detailed results\n",
        "    correct_sources = []\n",
        "    correct_targets = []\n",
        "    correct_predictions = []\n",
        "\n",
        "    incorrect_sources = []\n",
        "    incorrect_targets = []\n",
        "    incorrect_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, src_lengths, tgt in data_loader:\n",
        "            src, src_lengths, tgt = (x.to(device) for x in (src, src_lengths, tgt))\n",
        "            predictions = model.generate(src, src_lengths, tgt_vocab, max_len=tgt.size(1))\n",
        "\n",
        "            # Process each example in the batch\n",
        "            for idx in range(src.size(0)):\n",
        "                # Convert indices to strings\n",
        "                predicted_text = tgt_vocab.detokenize(predictions[idx].cpu().tolist())\n",
        "                target_text = tgt_vocab.detokenize(tgt[idx, 1:].cpu().tolist())  # Skip <bos>\n",
        "                source_text = src_vocab.detokenize(src[idx].cpu().tolist())\n",
        "\n",
        "                # Check if prediction matches target\n",
        "                is_correct = (predicted_text == target_text)\n",
        "                correct += is_correct\n",
        "\n",
        "                # Store detailed results\n",
        "                if is_correct:\n",
        "                    correct_sources.append(source_text)\n",
        "                    correct_targets.append(target_text)\n",
        "                    correct_predictions.append(predicted_text)\n",
        "                else:\n",
        "                    incorrect_sources.append(source_text)\n",
        "                    incorrect_targets.append(target_text)\n",
        "                    incorrect_predictions.append(predicted_text)\n",
        "\n",
        "            total += src.size(0)\n",
        "\n",
        "    accuracy = correct / total if total else 0.0\n",
        "    return (\n",
        "        accuracy,\n",
        "        (correct_sources, correct_targets, correct_predictions),\n",
        "        (incorrect_sources, incorrect_targets, incorrect_predictions)\n",
        "    )\n",
        "\n",
        "def save_predictions(src_list, tgt_list, pred_list, file_name):\n",
        "    \"\"\"Save prediction details to CSV file for analysis\"\"\"\n",
        "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Source', 'Target', 'Predicted'])\n",
        "        for row in zip(src_list, tgt_list, pred_list):\n",
        "            writer.writerow(row)\n",
        "\n",
        "    return file_name\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    data_loaders,\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device,\n",
        "    config,\n",
        "    save_path=None,\n",
        "    log_to_wandb=True\n",
        "):\n",
        "    \"\"\"Train a sequence-to-sequence model with attention\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_id)\n",
        "\n",
        "    # Select optimizer based on config\n",
        "    if config['optimizer'].lower() == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    elif config['optimizer'].lower() == 'nadam':\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=config['learning_rate'])\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "    # Track best validation accuracy\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in tqdm(range(1, config['epochs'] + 1), desc=\"Epochs\", position=0):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Training batches with progress bar\n",
        "        train_loader = tqdm(data_loaders['train'], desc=f\"Train {epoch}\", leave=False, position=1)\n",
        "        for src, src_lengths, tgt in train_loader:\n",
        "            src, src_lengths, tgt = src.to(device), src_lengths.to(device), tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, src_lengths, tgt, teacher_forcing_ratio=config['teacher_forcing'])\n",
        "            loss = criterion(output.reshape(-1, output.size(-1)), tgt[:,1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_loader.close()\n",
        "        train_loss = total_loss / len(data_loaders['train'])\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_loader = tqdm(data_loaders['dev'], desc=f\"Val {epoch}\", leave=False, position=1)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for src, src_lengths, tgt in val_loader:\n",
        "                src, src_lengths, tgt = src.to(device), src_lengths.to(device), tgt.to(device)\n",
        "                output = model(src, src_lengths, tgt, teacher_forcing_ratio=0.0)  # No teacher forcing in validation\n",
        "                val_loss += criterion(output.reshape(-1, output.size(-1)),\n",
        "                                    tgt[:,1:].reshape(-1)).item()\n",
        "        val_loader.close()\n",
        "        val_loss /= len(data_loaders['dev'])\n",
        "\n",
        "        # Compute accuracy metrics\n",
        "        train_results = calculate_accuracy(model, data_loaders['train'], tgt_vocab, src_vocab, device)\n",
        "        train_acc = train_results[0]\n",
        "\n",
        "        val_results = calculate_accuracy(model, data_loaders['dev'], tgt_vocab, src_vocab, device)\n",
        "        val_acc = val_results[0]\n",
        "\n",
        "        # Save model if it's the best so far\n",
        "        if val_acc > best_val_acc and save_path:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"New best model saved with validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "            # Save prediction analysis for milestone epochs\n",
        "            if epoch == config['epochs'] or epoch % 5 == 0:\n",
        "                correct_data = val_results[1]\n",
        "                incorrect_data = val_results[2]\n",
        "\n",
        "                save_predictions(\n",
        "                    correct_data[0], correct_data[1], correct_data[2],\n",
        "                    f\"correct_predictions_epoch_{epoch}.csv\"\n",
        "                )\n",
        "\n",
        "                save_predictions(\n",
        "                    incorrect_data[0], incorrect_data[1], incorrect_data[2],\n",
        "                    f\"incorrect_predictions_epoch_{epoch}.csv\"\n",
        "                )\n",
        "\n",
        "        # Log metrics\n",
        "        print(f\"Epoch {epoch}/{config['epochs']}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if log_to_wandb:\n",
        "            wandb.log({\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'validation_loss': val_loss,\n",
        "                'train_accuracy': train_acc,\n",
        "                'validation_accuracy': val_acc\n",
        "            })\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    test_results = calculate_accuracy(model, data_loaders['test'], tgt_vocab, src_vocab, device)\n",
        "    test_acc = test_results[0]\n",
        "    print(f\"Final test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    if log_to_wandb:\n",
        "        wandb.log({'test_accuracy': test_acc})\n",
        "\n",
        "    # Save final prediction analysis\n",
        "    correct_data = test_results[1]\n",
        "    incorrect_data = test_results[2]\n",
        "\n",
        "    save_predictions(\n",
        "        correct_data[0], correct_data[1], correct_data[2],\n",
        "        \"correct_predictions_final.csv\"\n",
        "    )\n",
        "\n",
        "    save_predictions(\n",
        "        incorrect_data[0], incorrect_data[1], incorrect_data[2],\n",
        "        \"incorrect_predictions_final.csv\"\n",
        "    )\n",
        "\n",
        "    return model, test_acc\n",
        "\n",
        "# Hyperparameter sweep configuration\n",
        "def get_sweep_config():\n",
        "    \"\"\"Define the hyperparameter sweep configuration for wandb\"\"\"\n",
        "    sweep_config = {\n",
        "        'method': 'bayes',  # Use Bayesian optimization\n",
        "        'name': 'Transliteration_with_Attention',\n",
        "        'metric': {'name': 'validation_accuracy', 'goal': 'maximize'},\n",
        "        'parameters': {\n",
        "            # Model architecture\n",
        "            'embedding_dim': {'values': [128, 256, 512]},\n",
        "            'hidden_dim': {'values': [128, 256, 512, 1024]},\n",
        "            'num_layers': {'values': [1, 2, 3, 4]},\n",
        "            'rnn_type': {'values': ['RNN', 'GRU', 'LSTM']},\n",
        "            'bidirectional': {'values': [True, False]},\n",
        "\n",
        "            # Training parameters\n",
        "            'dropout': {'values': [0.0, 0.1, 0.2, 0.3, 0.5]},\n",
        "            'learning_rate': {'values': [1e-4, 2e-4, 5e-4, 8e-4, 1e-3]},\n",
        "            'batch_size': {'values': [32, 64, 128]},\n",
        "            'epochs': {'values': [10, 15, 20]},\n",
        "            'teacher_forcing': {'values': [0.3, 0.5, 0.7, 1.0]},\n",
        "            'optimizer': {'values': ['Adam', 'NAdam']},\n",
        "            'seed': {'values': [42, 43, 44, 45, 46]},\n",
        "        }\n",
        "    }\n",
        "    return sweep_config\n",
        "\n",
        "def run_sweep_objective():\n",
        "    \"\"\"Objective function for wandb sweep\"\"\"\n",
        "    # Initialize wandb run and get config\n",
        "    run = wandb.init()\n",
        "    config = wandb.config\n",
        "\n",
        "    # Convert to a normal dictionary for our function\n",
        "    experiment_config = {\n",
        "        'language': 'te',  # Telugu\n",
        "        'rnn_type': config.rnn_type,\n",
        "        'embedding_dim': config.embedding_dim,\n",
        "        'hidden_dim': config.hidden_dim,\n",
        "        'num_layers': config.num_layers,\n",
        "        'dropout': config.dropout,\n",
        "        'bidirectional': config.bidirectional,\n",
        "        'batch_size': config.batch_size,\n",
        "        'epochs': config.epochs,\n",
        "        'learning_rate': config.learning_rate,\n",
        "        'teacher_forcing': config.teacher_forcing,\n",
        "        'optimizer': config.optimizer,\n",
        "        'seed': config.seed\n",
        "    }\n",
        "\n",
        "    # Set seeds for reproducibility\n",
        "    set_random_seeds(experiment_config['seed'])\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Download and extract the dataset if necessary\n",
        "    if not os.path.exists('/content/dakshina_dataset_v1.0'):\n",
        "        print(\"Downloading Dakshina dataset...\")\n",
        "        !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "        !tar xopf dakshina_dataset_v1.0.tar\n",
        "\n",
        "    # Create a unique run name based on config\n",
        "    run_name = f\"{experiment_config['rnn_type']}_{experiment_config['num_layers']}l_{experiment_config['embedding_dim']}e_{experiment_config['hidden_dim']}h_\" \\\n",
        "               f\"{'bid' if experiment_config['bidirectional'] else 'uni'}_{experiment_config['dropout']}d_\" \\\n",
        "               f\"{experiment_config['teacher_forcing']}tf_{experiment_config['optimizer']}\"\n",
        "    wandb.run.name = run_name\n",
        "\n",
        "    # Load data\n",
        "    print(f\"Loading {experiment_config['language']} data...\")\n",
        "    data_loaders, src_vocab, tgt_vocab = load_data(\n",
        "        language=experiment_config['language'],\n",
        "        batch_size=experiment_config['batch_size'],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Create model components\n",
        "    print(\"Building model with attention...\")\n",
        "    encoder = RNNEncoder(\n",
        "        src_vocab.vocab_size,\n",
        "        experiment_config['embedding_dim'],\n",
        "        experiment_config['hidden_dim'],\n",
        "        num_layers=experiment_config['num_layers'],\n",
        "        rnn_type=experiment_config['rnn_type'],\n",
        "        dropout=experiment_config['dropout'],\n",
        "        bidirectional=experiment_config['bidirectional']\n",
        "    ).to(device)\n",
        "\n",
        "    # Calculate encoder output dimension (doubled if bidirectional)\n",
        "    encoder_output_dim = experiment_config['hidden_dim'] * 2 if experiment_config['bidirectional'] else experiment_config['hidden_dim']\n",
        "\n",
        "    decoder = AttentionDecoder(\n",
        "        tgt_vocab.vocab_size,\n",
        "        experiment_config['embedding_dim'],\n",
        "        encoder_output_dim,\n",
        "        experiment_config['hidden_dim'],\n",
        "        num_layers=experiment_config['num_layers'],\n",
        "        rnn_type=experiment_config['rnn_type'],\n",
        "        dropout=experiment_config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    model = Seq2SeqWithAttention(encoder, decoder, pad_idx=src_vocab.pad_id, device=device).to(device)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training model...\")\n",
        "    model_save_path = f\"model_{run_name}.pt\"\n",
        "\n",
        "    model, test_acc = train_model(\n",
        "        model=model,\n",
        "        data_loaders=data_loaders,\n",
        "        src_vocab=src_vocab,\n",
        "        tgt_vocab=tgt_vocab,\n",
        "        device=device,\n",
        "        config=experiment_config,\n",
        "        save_path=model_save_path,\n",
        "        log_to_wandb=True\n",
        "    )\n",
        "\n",
        "    # Wandb finish happens automatically when this function returns\n",
        "\n",
        "def run_transliteration_experiment(config=None, use_wandb=True, run_sweep=False, sweep_count=20):\n",
        "    \"\"\"Run a transliteration experiment with attention\"\"\"\n",
        "\n",
        "    if run_sweep:\n",
        "        # Run a hyperparameter sweep\n",
        "        sweep_config = get_sweep_config()\n",
        "        sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment_3\")\n",
        "        wandb.agent(sweep_id, function=run_sweep_objective, count=sweep_count)\n",
        "        return None, None, None\n",
        "\n",
        "    # Run a single experiment with the given config\n",
        "    if config is None:\n",
        "        config = {\n",
        "            'language': 'te',  # Telugu\n",
        "            'rnn_type': 'LSTM',\n",
        "            'embedding_dim': 256,\n",
        "            'hidden_dim': 512,\n",
        "            'num_layers': 2,\n",
        "            'dropout': 0.3,\n",
        "            'bidirectional': True,\n",
        "            'batch_size': 64,\n",
        "            'epochs': 10,\n",
        "            'learning_rate': 0.001,\n",
        "            'teacher_forcing': 0.5,\n",
        "            'optimizer': 'Adam',\n",
        "            'seed': 42\n",
        "        }\n",
        "\n",
        "    # Set seeds for reproducibility\n",
        "    set_random_seeds(config['seed'])\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize wandb\n",
        "    if use_wandb:\n",
        "        run_name = f\"{config['rnn_type']}_{config['num_layers']}l_{config['embedding_dim']}e_{config['hidden_dim']}h_\" \\\n",
        "                  f\"{'bid' if config['bidirectional'] else 'uni'}_{config['dropout']}d_\" \\\n",
        "                  f\"{config['teacher_forcing']}tf_{config['optimizer']}\"\n",
        "\n",
        "        wandb.init(\n",
        "            project=\"DA6401_Assignment_3\",\n",
        "            name=run_name,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "    # Download and extract the dataset if necessary\n",
        "    if not os.path.exists('/content/dakshina_dataset_v1.0'):\n",
        "        print(\"Downloading Dakshina dataset...\")\n",
        "        !wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "        !tar xopf dakshina_dataset_v1.0.tar\n",
        "\n",
        "    # Load data\n",
        "    print(f\"Loading {config['language']} data...\")\n",
        "    data_loaders, src_vocab, tgt_vocab = load_data(\n",
        "        language=config['language'],\n",
        "        batch_size=config['batch_size'],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Create model components\n",
        "    print(\"Building model with attention...\")\n",
        "    encoder = RNNEncoder(\n",
        "        src_vocab.vocab_size,\n",
        "        config['embedding_dim'],\n",
        "        config['hidden_dim'],\n",
        "        num_layers=config['num_layers'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        dropout=config['dropout'],\n",
        "        bidirectional=config['bidirectional']\n",
        "    ).to(device)\n",
        "\n",
        "    # Calculate encoder output dimension (doubled if bidirectional)\n",
        "    encoder_output_dim = config['hidden_dim'] * 2 if config['bidirectional'] else config['hidden_dim']\n",
        "\n",
        "    # Create attention decoder - explicit attention model\n",
        "    decoder = AttentionDecoder(\n",
        "        tgt_vocab.vocab_size,\n",
        "        config['embedding_dim'],\n",
        "        encoder_output_dim,\n",
        "        config['hidden_dim'],\n",
        "        num_layers=config['num_layers'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        dropout=config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    model = Seq2SeqWithAttention(encoder, decoder, pad_idx=src_vocab.pad_id, device=device).to(device)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training model with attention...\")\n",
        "    model_save_path = f\"transliteration_model_attention_{config['language']}_{config['rnn_type']}.pt\"\n",
        "\n",
        "    model, test_acc = train_model(\n",
        "        model=model,\n",
        "        data_loaders=data_loaders,\n",
        "        src_vocab=src_vocab,\n",
        "        tgt_vocab=tgt_vocab,\n",
        "        device=device,\n",
        "        config=config,\n",
        "        save_path=model_save_path,\n",
        "        log_to_wandb=use_wandb\n",
        "    )\n",
        "\n",
        "    print(f\"Training complete! Final test accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Finish the wandb run\n",
        "    if use_wandb:\n",
        "        wandb.finish()\n",
        "\n",
        "    return model, src_vocab, tgt_vocab\n",
        "\n",
        "# Run the experiment when executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    # Run a single experiment with attention\n",
        "    run_transliteration_experiment(use_wandb=True)\n",
        "\n",
        "    # To run a hyperparameter sweep:\n",
        "    # run_transliteration_experiment(use_wandb=True, run_sweep=True, sweep_count=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "By1JfgWf52lB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}